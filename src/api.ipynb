{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing.database import Database\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Database('../data/database.db')\n",
    "cmt = db.query('SELECT * FROM comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cmt.info(verbose = True))\n",
    "print(cmt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = cmt.describe().transpose()\n",
    "out[out['std'] == 0].transpose() # std == 0 means that these are likely useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cmt['num_sentences'].describe())\n",
    "cmt['num_sentences'].hist(bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cmt[cmt['num_sentences'] >= 3]['num_sentences']).count() / cmt['num_sentences'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = cmt[cmt['score'] == cmt['score'].min()]\n",
    "print(v)\n",
    "print(v['permalink'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmt[cmt['controversiality'] == cmt['controversiality'].max()]['permalink'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = cmt[cmt['num_sentences'] == cmt['num_sentences'].max()]\n",
    "v['permalink'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cmt['distinguished'].value_counts())\n",
    "print(cmt[cmt['author'] != 'AutoModerator']['distinguished'].value_counts())\n",
    "\n",
    "print(cmt[cmt['distinguished'] == 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import simpletransformers.classification as cl\n",
    "import re\n",
    "\n",
    "p = stanza.Pipeline(lang=\"en\", processors=\"tokenize\")\n",
    "\n",
    "count = len(cmt['body'])\n",
    "\n",
    "buf_docid = []\n",
    "buf_id = []\n",
    "buf_text = []\n",
    "\n",
    "for n in range(count):\n",
    "    if n % 10_000 == 0:\n",
    "        print(f'\\rCompletion = {n / count}', end='')\n",
    "\n",
    "    if cmt['num_sentences'][n] < 3:\n",
    "        continue\n",
    "    \n",
    "    # note that this gets rid of punctuation\n",
    "    sentenceDelimiter = re.compile(r'((?:\\.|\\?|!|\\S$)(?:\\s|$))', flags = re.MULTILINE)\n",
    "\n",
    "    one_text = [x.strip() for x in re.split(sentenceDelimiter, cmt['body'][n])]\n",
    "    if not one_text[-1]:\n",
    "        del one_text[-1]\n",
    "\n",
    "    if len(one_text) % 2 == 1:\n",
    "        raise Exception\n",
    "\n",
    "    c = int(len(one_text) / 2)\n",
    "    for t in range(c):\n",
    "        buf_docid.append(n + 1)\n",
    "        buf_id.append(t + 1)\n",
    "        buf_text.append(one_text[2 * t] + one_text[2 * t + 1])\n",
    "\n",
    "sentences_final = pd.DataFrame({\n",
    "    'doc_id': buf_docid,\n",
    "    'id': buf_id,\n",
    "    'text': buf_text\n",
    "})\n",
    "\n",
    "buf_docid = []\n",
    "buf_id = []\n",
    "buf_text = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrangle extracted sentences into sentence trigrams\n",
    "u_docs = sentences_final[\"doc_id\"].unique()\n",
    "\n",
    "buf_tri = []\n",
    "buf_docid = []\n",
    "buf_id = []\n",
    "\n",
    "cp = sentences_final['doc_id']\n",
    "tt = sentences_final['text']\n",
    "\n",
    "jj = len(cp)\n",
    "\n",
    "count = len(u_docs)\n",
    "j = 0\n",
    "for n in range(count):\n",
    "    if n % 10_000 == 0:\n",
    "        print(f'\\rCompletion = {n / count}', end='')\n",
    "\n",
    "    doc_id = u_docs[n]\n",
    "    text = []\n",
    "    while j < jj and cp[j] == doc_id:\n",
    "        text.append(tt[j])\n",
    "        j += 1\n",
    "\n",
    "    if len(text) < 3:\n",
    "        sentence_trigram = [' '.join(text)]\n",
    "    else:\n",
    "        # create trigrams\n",
    "        sentence_trigram = []\n",
    "        for i in range(len(text) - 2):\n",
    "            sentence_trigram.append(text[i] + ' ' + text[i + 1] + ' ' + text[i + 2])\n",
    "\n",
    "    for i in range(len(sentence_trigram)):\n",
    "        buf_tri.append(sentence_trigram[i])\n",
    "        buf_id.append(i + 1)\n",
    "        buf_docid.append(doc_id)\n",
    "\n",
    "# concatenate all the new data into a final DataFrame\n",
    "final = pd.DataFrame({\n",
    "    'sent_trigram': buf_tri,\n",
    "    'id': buf_id,\n",
    "    'doc_id': buf_docid,\n",
    "})\n",
    "\n",
    "buf_tri = []\n",
    "buf_docid = []\n",
    "buf_id = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of comments: {len(u_docs)}\")\n",
    "print(f'Number of trigrams: {len(final['sent_trigram'])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load authdetect model and predict (don't use multiprocessing for larger text since it causes stalling)\n",
    "model = cl.ClassificationModel(\"roberta\",\n",
    "                               \"mmochtak/authdetect\", \n",
    "                               args={\"use_multiprocessing_for_evaluation\": True,},\n",
    "                               use_cuda=False\n",
    "                               )\n",
    "\n",
    "# Annotate the prepared trigrams with the authdetect model.\n",
    "prediction = model.predict(to_predict = final[\"sent_trigram\"].tolist())\n",
    "\n",
    "anno_df = final.assign(predict = prediction[1])\n",
    "\n",
    "scores = prediction[1] if isinstance(prediction, tuple) and len(prediction) >= 2 else prediction\n",
    "\n",
    "anno_df_speech = anno_df.groupby('doc_id').agg(\n",
    "    demo=('predict', lambda x: x.mean()),\n",
    "    auth=('predict', lambda x: 1 - x.mean()),\n",
    "    auth_sent=('predict', lambda x: (x <= 0.5).mean()),\n",
    "    num_sent=('predict', lambda x: x.size),\n",
    "    predict_std=('predict', lambda x: x.std(ddof=0)),\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_df_speech['auth'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = []\n",
    "for i in range(len(anno_df_speech['doc_id'])):\n",
    "    doc_id = int(anno_df_speech['doc_id'][i])\n",
    "    row = cmt.loc[doc_id - 1, :]\n",
    "    dd.append(row['id'])\n",
    "\n",
    "anno_df_speech = anno_df_speech.assign(cmt_id = dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "out = {\n",
    "    'anno_df.db': anno_df,\n",
    "    'anno_df_speech.db': anno_df_speech\n",
    "}\n",
    "\n",
    "for file, data in out.items():\n",
    "    open(file, 'w').close()\n",
    "    con = sqlite3.connect(file)\n",
    "    data.to_sql('main', con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              index        doc_id          demo          auth     auth_sent  \\\n",
      "count  25431.000000  25431.000000  25431.000000  25431.000000  25431.000000   \n",
      "mean   12715.000000  50005.606386      0.652021      0.347979      0.133679   \n",
      "std     7341.441684  28996.542979      0.125004      0.125004      0.293252   \n",
      "min        0.000000      1.000000      0.115170      0.067089      0.000000   \n",
      "25%     6357.500000  24780.500000      0.588456      0.259382      0.000000   \n",
      "50%    12715.000000  50026.000000      0.673541      0.326459      0.000000   \n",
      "75%    19072.500000  75004.000000      0.740618      0.411544      0.000000   \n",
      "max    25430.000000  99996.000000      0.932911      0.884830      1.000000   \n",
      "\n",
      "           num_sent   predict_std  \n",
      "count  25431.000000  25431.000000  \n",
      "mean       3.703197      0.038484  \n",
      "std        5.243605      0.047764  \n",
      "min        1.000000      0.000000  \n",
      "25%        1.000000      0.000000  \n",
      "50%        2.000000      0.021162  \n",
      "75%        4.000000      0.062102  \n",
      "max      184.000000      0.337913  \n",
      "       index  doc_id      demo      auth  auth_sent  num_sent  predict_std  \\\n",
      "10190  10190   39768  0.932911  0.067089        0.0         1          0.0   \n",
      "\n",
      "        cmt_id  \n",
      "10190  n0o7xi6  \n",
      "Doesn’t change the fact that justice is being served. The appeals for a death penalty can be over 20 years. And it would definitely me on the higher end for such a high profile case like this.\n",
      "/r/idahomurders/comments/1lomb44/why_would_prosecutors_even_offer_kohberger_a_plea/n0o7xi6/\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from processing.database import Database\n",
    "\n",
    "con = sqlite3.connect('anno_df_speech.db')\n",
    "con.cursor()\n",
    "\n",
    "anno_df_speech = pd.read_sql('SELECT * FROM main;', con)\n",
    "\n",
    "cmt = Database('../data/database.db').query('SELECT * FROM comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              index        doc_id          demo          auth     auth_sent  \\\n",
      "count  25431.000000  25431.000000  25431.000000  25431.000000  25431.000000   \n",
      "mean   12715.000000  50005.606386      0.652021      0.347979      0.133679   \n",
      "std     7341.441684  28996.542979      0.125004      0.125004      0.293252   \n",
      "min        0.000000      1.000000      0.115170      0.067089      0.000000   \n",
      "25%     6357.500000  24780.500000      0.588456      0.259382      0.000000   \n",
      "50%    12715.000000  50026.000000      0.673541      0.326459      0.000000   \n",
      "75%    19072.500000  75004.000000      0.740618      0.411544      0.000000   \n",
      "max    25430.000000  99996.000000      0.932911      0.884830      1.000000   \n",
      "\n",
      "           num_sent   predict_std  \n",
      "count  25431.000000  25431.000000  \n",
      "mean       3.703197      0.038484  \n",
      "std        5.243605      0.047764  \n",
      "min        1.000000      0.000000  \n",
      "25%        1.000000      0.000000  \n",
      "50%        2.000000      0.021162  \n",
      "75%        4.000000      0.062102  \n",
      "max      184.000000      0.337913  \n",
      "       index  doc_id      demo      auth  auth_sent  num_sent  predict_std  \\\n",
      "10190  10190   39768  0.932911  0.067089        0.0         1          0.0   \n",
      "\n",
      "        cmt_id  \n",
      "10190  n0o7xi6  \n",
      "       num_sentences  edited  removal_type  collapsed  distinguished  \\\n",
      "39767              3       0             1          3              0   \n",
      "\n",
      "       subreddit_type              author  \\\n",
      "39767               0  Aggressive_Opossum   \n",
      "\n",
      "                                                    body  created_utc  \\\n",
      "39767  Doesn’t change the fact that justice is being ...   1751328292   \n",
      "\n",
      "       archived  ...       id     link_id locked  is_submitter   parent_id  \\\n",
      "39767         0  ...  n0o7xi6  t3_1lomb44      0             0  t1_n0o60y0   \n",
      "\n",
      "      score  subreddit_id     subreddit stickied  \\\n",
      "39767     1     t5_7elrxg  idahomurders        0   \n",
      "\n",
      "                                               permalink  \n",
      "39767  /r/idahomurders/comments/1lomb44/why_would_pro...  \n",
      "\n",
      "[1 rows x 21 columns]\n",
      "Doesn’t change the fact that justice is being served. The appeals for a death penalty can be over 20 years. And it would definitely me on the higher end for such a high profile case like this.\n",
      "/r/idahomurders/comments/1lomb44/why_would_prosecutors_even_offer_kohberger_a_plea/n0o7xi6/\n"
     ]
    }
   ],
   "source": [
    "print(anno_df_speech.describe())\n",
    "m = anno_df_speech['auth'].min()\n",
    "v = anno_df_speech[anno_df_speech['auth'] == m]\n",
    "print(v)\n",
    "most = cmt[cmt['id'] == v['cmt_id'].values[0]]\n",
    "print(most)\n",
    "print(most['body'].values[0])\n",
    "print(most['permalink'].values[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
